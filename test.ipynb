{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from alpaca.data.historical.news import NewsClient\n",
    "from alpaca.data.requests import NewsRequest\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load API credential from YAML configuration file\n",
    "print('Loading API keys...')\n",
    "\n",
    "with open(\"data/conf/keys.yaml\", \"r\") as f:\n",
    "    keys = yaml.safe_load(f)\n",
    "\n",
    "API_KEY = keys['KEYS']['APCA-API-KEY-ID-Data']\n",
    "SECRET_KEY = keys['KEYS']['APCA-API-SECRET-KEY-Data']\n",
    "\n",
    "print(\"API keys loaded.\")\n",
    "\n"
   ],
   "id": "f08a97c4b0916f2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from alpaca.data.historical import NewsClient\n",
    "from alpaca.data.requests import NewsRequest\n",
    "\n",
    "print('Downloading news data...')\n",
    "\n",
    "news_client = NewsClient(\n",
    "    api_key=API_KEY,\n",
    "    secret_key=SECRET_KEY\n",
    ")\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime.now()\n"
   ],
   "id": "77ba7a4d6810bba5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_articles = []\n",
    "\n",
    "current_start = start_date\n",
    "block_size = timedelta(days=7)  # 7-Tage-Blöcke\n",
    "\n",
    "while current_start < end_date:\n",
    "    current_end = min(current_start + block_size, end_date)\n",
    "\n",
    "    request = NewsRequest(\n",
    "        symbols=\"APPL\",\n",
    "        start=current_start,\n",
    "        end=current_end,\n",
    "        limit=50  # max pro Request\n",
    "    )\n",
    "\n",
    "    articles = news_client.get_news(request)\n",
    "    articles_list = list(articles)  # NewsSet → list\n",
    "\n",
    "    if articles_list:\n",
    "        all_articles.extend(articles_list)\n",
    "        print(f\"Fetched {len(articles_list)} articles from {current_start.date()} to {current_end.date()}\")\n",
    "\n",
    "    current_start = current_end  # nächster Block\n",
    "\n",
    "print(f\"Total articles fetched: {len(all_articles)}\")\n"
   ],
   "id": "dd437dcf2e240c7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Headlines, Publish Time und URL extrahieren\n",
    "data = []\n",
    "for item in all_articles:\n",
    "    # Falls item ein Dict ist\n",
    "    if isinstance(item, dict):\n",
    "        data.append({\n",
    "            \"headline\": item.get(\"headline\", \"\"),\n",
    "            \"published_at\": item.get(\"published_at\", \"\"),\n",
    "            \"url\": item.get(\"url\", \"\")\n",
    "        })\n",
    "    # Falls es ein String ist (sehr selten)\n",
    "    else:\n",
    "        data.append({\n",
    "            \"headline\": str(item),\n",
    "            \"published_at\": \"\",\n",
    "            \"url\": \"\"\n",
    "        })\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# CSV speichern\n",
    "csv_path = 'congress_news.csv' #\"apple_news_jan2022_to_now.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"CSV saved as: {csv_path}\")\n",
    "df.head()"
   ],
   "id": "db66cc3851030e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Debug: Let's see what's actually in all_articles\n",
    "print(f\"Type of all_articles: {type(all_articles)}\")\n",
    "print(f\"Length of all_articles: {len(all_articles)}\")\n",
    "\n",
    "if all_articles:\n",
    "    print(f\"Type of first item: {type(all_articles[0])}\")\n",
    "    print(f\"First item length: {len(all_articles[0])}\")\n",
    "\n",
    "# Process the tuples - they are in format ('data', {'news': [...]})\n",
    "flattened_data = []\n",
    "\n",
    "for item in all_articles:\n",
    "    if isinstance(item, tuple) and len(item) == 2:\n",
    "        if item[0] == 'data' and isinstance(item[1], dict) and 'news' in item[1]:\n",
    "            # Extract news from the dictionary\n",
    "            news_list = item[1]['news']\n",
    "            for article in news_list:\n",
    "                # Access attributes directly from the News object\n",
    "                created_at = article.created_at\n",
    "                if hasattr(created_at, 'strftime'):\n",
    "                    created_at = created_at.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                flattened_data.append({\n",
    "                    'id': article.id,\n",
    "                    'headline': article.headline,\n",
    "                    'created_at': created_at,\n",
    "                    'url': article.url\n",
    "\n",
    "                })\n",
    "\n",
    "print(f\"Processed {len(flattened_data)} articles\")\n",
    "\n",
    "# Create DataFrame\n",
    "clean_df = pd.DataFrame(flattened_data)\n",
    "\n",
    "print(f\"Created DataFrame with {len(clean_df)} articles\")\n",
    "\n",
    "# Save to CSV\n",
    "clean_df.to_csv(\"CLEAN_congress_news.csv\", index=False, encoding='utf-8')\n",
    "print(\"CSV saved successfully!\")\n",
    "\n",
    "# Display the first few rows\n",
    "if not clean_df.empty:\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(clean_df.head())\n",
    "else:\n",
    "    print(\"DataFrame is empty\")"
   ],
   "id": "feaabe6ebc41a731",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lukekerbs/us-senate-financial-disclosures-stocks-and-options\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "64e670d2204350e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Datei einlesen\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Zeige die Spaltennamen\n",
    "print(df.columns)\n"
   ],
   "id": "7dbfc497022dedeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ticker_counts = df.groupby('ticker').agg({\n",
    "    'asset_name': lambda x: ', '.join(x.unique()),\n",
    "    'ticker': 'size'\n",
    "}).rename(columns={'ticker': 'Count'}).sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(ticker_counts.head(20))\n"
   ],
   "id": "60364cd20ecd5114",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:55:44.995795Z",
     "start_time": "2025-11-30T13:32:24.411058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" Scrape stock transactions from Senator periodic filings (resumable + ETA logging) \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "ROOT = 'https://efdsearch.senate.gov'\n",
    "LANDING_PAGE_URL = f'{ROOT}/search/home/'\n",
    "SEARCH_PAGE_URL = f'{ROOT}/search/'\n",
    "REPORTS_URL = f'{ROOT}/search/report/data/'\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "RATE_LIMIT_SECS = 2\n",
    "PDF_PREFIX = '/search/view/paper/'\n",
    "OUTPUT_CSV = 'senator_transactions_all.csv'\n",
    "MAX_RETRIES = 3\n",
    "TIMEOUT = 10\n",
    "\n",
    "REPORT_COL_NAMES = [\n",
    "    'tx_date',\n",
    "    'file_date',\n",
    "    'last_name',\n",
    "    'first_name',\n",
    "    'order_type',\n",
    "    'ticker',\n",
    "    'asset_name',\n",
    "    'tx_amount',\n",
    "    'link'\n",
    "]\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "def add_rate_limit(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        time.sleep(RATE_LIMIT_SECS)\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "def _csrf(client: requests.Session) -> str:\n",
    "    landing_page_response = client.get(LANDING_PAGE_URL)\n",
    "    assert landing_page_response.url == LANDING_PAGE_URL, \"Failed to fetch filings landing page\"\n",
    "    landing_page = BeautifulSoup(landing_page_response.text, \"html.parser\")\n",
    "    form_csrf = landing_page.find(attrs={'name': 'csrfmiddlewaretoken'})['value']\n",
    "\n",
    "    client.post(LANDING_PAGE_URL,\n",
    "                data={'csrfmiddlewaretoken': form_csrf, 'prohibition_agreement': '1'},\n",
    "                headers={'Referer': LANDING_PAGE_URL})\n",
    "\n",
    "    return client.cookies.get('csrftoken') or client.cookies.get('csrf')\n",
    "\n",
    "def reports_api(client: requests.Session, start_date: str, end_date: str, token: str):\n",
    "    data = {\n",
    "        'start': '0',  # immer vom Anfang\n",
    "        'length': str(BATCH_SIZE),\n",
    "        'report_types': '[11]',\n",
    "        'filer_types': '[]',\n",
    "        'submitted_start_date': start_date,\n",
    "        'submitted_end_date': end_date,\n",
    "        'candidate_state': '',\n",
    "        'senator_state': '',\n",
    "        'office_id': '',\n",
    "        'first_name': '',\n",
    "        'last_name': '',\n",
    "        'csrfmiddlewaretoken': token\n",
    "    }\n",
    "    LOGGER.info(f'Getting reports from {start_date} to {end_date}')\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            resp = client.post(REPORTS_URL, data=data, headers={'Referer': SEARCH_PAGE_URL}, timeout=TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()['data']\n",
    "        except Exception as e:\n",
    "            LOGGER.warning(f'Attempt {attempt+1} failed: {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    raise RuntimeError(f'Failed to fetch reports from {start_date} to {end_date} after {MAX_RETRIES} attempts')\n",
    "\n",
    "def _tbody_from_link(client: requests.Session, link: str):\n",
    "    report_url = f'{ROOT}{link}'\n",
    "    resp = client.get(report_url)\n",
    "    if resp.url == LANDING_PAGE_URL:\n",
    "        _csrf(client)\n",
    "        resp = client.get(report_url)\n",
    "    report = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    tbodies = report.find_all('tbody')\n",
    "    return tbodies[0] if tbodies else None\n",
    "\n",
    "def txs_for_report_all(client: requests.Session, row):\n",
    "    first, last, _, link_html, date_received = row\n",
    "    link_soup = BeautifulSoup(link_html, \"html.parser\")\n",
    "    a_tag = link_soup.a\n",
    "    link = a_tag.get('href') if a_tag else None\n",
    "\n",
    "    if not link or link.startswith(PDF_PREFIX):\n",
    "        return pd.DataFrame([{\n",
    "            'tx_date': None,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': None,\n",
    "            'ticker': None,\n",
    "            'asset_name': None,\n",
    "            'tx_amount': None,\n",
    "            'link': f\"{ROOT}{link}\" if link else None\n",
    "        }])\n",
    "\n",
    "    tbody = _tbody_from_link(client, link)\n",
    "    if not tbody:\n",
    "        return pd.DataFrame([{\n",
    "            'tx_date': None,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': None,\n",
    "            'ticker': None,\n",
    "            'asset_name': None,\n",
    "            'tx_amount': None,\n",
    "            'link': f\"{ROOT}{link}\"\n",
    "        }])\n",
    "\n",
    "    stocks = []\n",
    "    for tr in tbody.find_all('tr'):\n",
    "        cols = [c.get_text().strip() for c in tr.find_all('td')]\n",
    "        if len(cols) < 8:\n",
    "            continue\n",
    "        tx_date, ticker, asset_name, asset_type, order_type, tx_amount = \\\n",
    "            cols[1], cols[3], cols[4], cols[5], cols[6], cols[7]\n",
    "        if asset_type != 'Stock' and ticker.strip() in ('--', ''):\n",
    "            continue\n",
    "        stocks.append({\n",
    "            'tx_date': tx_date,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': order_type,\n",
    "            'ticker': ticker,\n",
    "            'asset_name': asset_name,\n",
    "            'tx_amount': tx_amount,\n",
    "            'link': f\"{ROOT}{link}\"\n",
    "        })\n",
    "    return pd.DataFrame(stocks)\n",
    "\n",
    "def main():\n",
    "    LOGGER.info('Initializing client')\n",
    "    client = requests.Session()\n",
    "    client.get = add_rate_limit(client.get)\n",
    "    client.post = add_rate_limit(client.post)\n",
    "\n",
    "    token = _csrf(client)\n",
    "\n",
    "    start = datetime(2012, 1, 1)\n",
    "    end = datetime.today()\n",
    "\n",
    "    # CSV vorbereiten\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        mode = 'a'\n",
    "        header = False\n",
    "    else:\n",
    "        mode = 'w'\n",
    "        header = True\n",
    "\n",
    "    while start < end:\n",
    "        month_end = (start.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(days=1)\n",
    "        start_str = start.strftime(\"%m/%d/%Y 00:00:00\")\n",
    "        end_str = month_end.strftime(\"%m/%d/%Y 23:59:59\")\n",
    "\n",
    "        batch = reports_api(client, start_str, end_str, token)\n",
    "\n",
    "        for r in batch:\n",
    "            df = txs_for_report_all(client, r)\n",
    "            df.to_csv(OUTPUT_CSV, mode=mode, header=header, index=False)\n",
    "            header = False\n",
    "            mode = 'a'\n",
    "\n",
    "        start = month_end + timedelta(days=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='[%(asctime)s %(levelname)s] %(message)s')\n",
    "    main()\n"
   ],
   "id": "43fec415edb9df5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-30 14:32:24,463 INFO] Initializing client\n",
      "[2025-11-30 14:32:29,499 INFO] Getting reports from 01/01/2012 00:00:00 to 01/31/2012 23:59:59\n",
      "[2025-11-30 14:32:31,797 INFO] Getting reports from 02/01/2012 00:00:00 to 02/29/2012 23:59:59\n",
      "[2025-11-30 14:32:34,135 INFO] Getting reports from 03/01/2012 00:00:00 to 03/31/2012 23:59:59\n",
      "[2025-11-30 14:32:36,484 INFO] Getting reports from 04/01/2012 00:00:00 to 04/30/2012 23:59:59\n",
      "[2025-11-30 14:32:38,889 INFO] Getting reports from 05/01/2012 00:00:00 to 05/31/2012 23:59:59\n",
      "[2025-11-30 14:32:41,227 INFO] Getting reports from 06/01/2012 00:00:00 to 06/30/2012 23:59:59\n",
      "[2025-11-30 14:32:43,574 INFO] Getting reports from 07/01/2012 00:00:00 to 07/31/2012 23:59:59\n",
      "[2025-11-30 14:32:45,998 INFO] Getting reports from 08/01/2012 00:00:00 to 08/31/2012 23:59:59\n",
      "[2025-11-30 14:32:48,347 INFO] Getting reports from 09/01/2012 00:00:00 to 09/30/2012 23:59:59\n",
      "[2025-11-30 14:32:50,687 INFO] Getting reports from 10/01/2012 00:00:00 to 10/31/2012 23:59:59\n",
      "[2025-11-30 14:32:53,059 INFO] Getting reports from 11/01/2012 00:00:00 to 11/30/2012 23:59:59\n",
      "[2025-11-30 14:32:55,381 INFO] Getting reports from 12/01/2012 00:00:00 to 12/31/2012 23:59:59\n",
      "[2025-11-30 14:32:57,718 INFO] Getting reports from 01/01/2013 00:00:00 to 01/31/2013 23:59:59\n",
      "[2025-11-30 14:33:00,058 INFO] Getting reports from 02/01/2013 00:00:00 to 02/28/2013 23:59:59\n",
      "[2025-11-30 14:33:02,468 INFO] Getting reports from 03/01/2013 00:00:00 to 03/31/2013 23:59:59\n",
      "[2025-11-30 14:33:04,908 INFO] Getting reports from 04/01/2013 00:00:00 to 04/30/2013 23:59:59\n",
      "[2025-11-30 14:33:07,335 INFO] Getting reports from 05/01/2013 00:00:00 to 05/31/2013 23:59:59\n",
      "[2025-11-30 14:33:09,718 INFO] Getting reports from 06/01/2013 00:00:00 to 06/30/2013 23:59:59\n",
      "[2025-11-30 14:33:12,083 INFO] Getting reports from 07/01/2013 00:00:00 to 07/31/2013 23:59:59\n",
      "[2025-11-30 14:33:14,519 INFO] Getting reports from 08/01/2013 00:00:00 to 08/31/2013 23:59:59\n",
      "[2025-11-30 14:33:17,297 INFO] Getting reports from 09/01/2013 00:00:00 to 09/30/2013 23:59:59\n",
      "[2025-11-30 14:33:20,099 INFO] Getting reports from 10/01/2013 00:00:00 to 10/31/2013 23:59:59\n",
      "[2025-11-30 14:33:22,543 INFO] Getting reports from 11/01/2013 00:00:00 to 11/30/2013 23:59:59\n",
      "[2025-11-30 14:33:24,927 INFO] Getting reports from 12/01/2013 00:00:00 to 12/31/2013 23:59:59\n",
      "[2025-11-30 14:33:27,281 INFO] Getting reports from 01/01/2014 00:00:00 to 01/31/2014 23:59:59\n",
      "[2025-11-30 14:33:34,164 INFO] Getting reports from 02/01/2014 00:00:00 to 02/28/2014 23:59:59\n",
      "[2025-11-30 14:33:55,577 INFO] Getting reports from 03/01/2014 00:00:00 to 03/31/2014 23:59:59\n",
      "[2025-11-30 14:34:16,698 INFO] Getting reports from 04/01/2014 00:00:00 to 04/30/2014 23:59:59\n",
      "[2025-11-30 14:34:49,652 INFO] Getting reports from 05/01/2014 00:00:00 to 05/31/2014 23:59:59\n",
      "[2025-11-30 14:35:13,095 INFO] Getting reports from 06/01/2014 00:00:00 to 06/30/2014 23:59:59\n",
      "[2025-11-30 14:35:33,916 INFO] Getting reports from 07/01/2014 00:00:00 to 07/31/2014 23:59:59\n",
      "[2025-11-30 14:35:55,942 INFO] Getting reports from 08/01/2014 00:00:00 to 08/31/2014 23:59:59\n",
      "[2025-11-30 14:36:22,245 INFO] Getting reports from 09/01/2014 00:00:00 to 09/30/2014 23:59:59\n",
      "[2025-11-30 14:36:43,455 INFO] Getting reports from 10/01/2014 00:00:00 to 10/31/2014 23:59:59\n",
      "[2025-11-30 14:36:57,392 INFO] Getting reports from 11/01/2014 00:00:00 to 11/30/2014 23:59:59\n",
      "[2025-11-30 14:37:11,144 INFO] Getting reports from 12/01/2014 00:00:00 to 12/31/2014 23:59:59\n",
      "[2025-11-30 14:37:36,925 INFO] Getting reports from 01/01/2015 00:00:00 to 01/31/2015 23:59:59\n",
      "[2025-11-30 14:38:03,005 INFO] Getting reports from 02/01/2015 00:00:00 to 02/28/2015 23:59:59\n",
      "[2025-11-30 14:38:28,260 INFO] Getting reports from 03/01/2015 00:00:00 to 03/31/2015 23:59:59\n",
      "[2025-11-30 14:39:05,662 INFO] Getting reports from 04/01/2015 00:00:00 to 04/30/2015 23:59:59\n",
      "[2025-11-30 14:39:41,922 INFO] Getting reports from 05/01/2015 00:00:00 to 05/31/2015 23:59:59\n",
      "[2025-11-30 14:40:31,412 INFO] Getting reports from 06/01/2015 00:00:00 to 06/30/2015 23:59:59\n",
      "[2025-11-30 14:41:03,962 INFO] Getting reports from 07/01/2015 00:00:00 to 07/31/2015 23:59:59\n",
      "[2025-11-30 14:41:56,093 INFO] Getting reports from 08/01/2015 00:00:00 to 08/31/2015 23:59:59\n",
      "[2025-11-30 14:42:53,096 INFO] Getting reports from 09/01/2015 00:00:00 to 09/30/2015 23:59:59\n",
      "[2025-11-30 14:43:46,702 INFO] Getting reports from 10/01/2015 00:00:00 to 10/31/2015 23:59:59\n",
      "[2025-11-30 14:44:16,678 INFO] Getting reports from 11/01/2015 00:00:00 to 11/30/2015 23:59:59\n",
      "[2025-11-30 14:44:44,251 INFO] Getting reports from 12/01/2015 00:00:00 to 12/31/2015 23:59:59\n",
      "[2025-11-30 14:45:27,573 INFO] Getting reports from 01/01/2016 00:00:00 to 01/31/2016 23:59:59\n",
      "[2025-11-30 14:46:14,555 INFO] Getting reports from 02/01/2016 00:00:00 to 02/29/2016 23:59:59\n",
      "[2025-11-30 14:46:59,820 INFO] Getting reports from 03/01/2016 00:00:00 to 03/31/2016 23:59:59\n",
      "[2025-11-30 14:47:44,357 INFO] Getting reports from 04/01/2016 00:00:00 to 04/30/2016 23:59:59\n",
      "[2025-11-30 14:48:20,730 INFO] Getting reports from 05/01/2016 00:00:00 to 05/31/2016 23:59:59\n",
      "[2025-11-30 14:49:03,609 INFO] Getting reports from 06/01/2016 00:00:00 to 06/30/2016 23:59:59\n",
      "[2025-11-30 14:49:29,575 INFO] Getting reports from 07/01/2016 00:00:00 to 07/31/2016 23:59:59\n",
      "[2025-11-30 14:50:05,463 INFO] Getting reports from 08/01/2016 00:00:00 to 08/31/2016 23:59:59\n",
      "[2025-11-30 14:51:18,513 INFO] Getting reports from 09/01/2016 00:00:00 to 09/30/2016 23:59:59\n",
      "[2025-11-30 14:51:54,561 INFO] Getting reports from 10/01/2016 00:00:00 to 10/31/2016 23:59:59\n",
      "[2025-11-30 14:52:28,001 INFO] Getting reports from 11/01/2016 00:00:00 to 11/30/2016 23:59:59\n",
      "[2025-11-30 14:52:57,479 INFO] Getting reports from 12/01/2016 00:00:00 to 12/31/2016 23:59:59\n",
      "[2025-11-30 14:53:38,981 INFO] Getting reports from 01/01/2017 00:00:00 to 01/31/2017 23:59:59\n",
      "[2025-11-30 14:54:12,242 INFO] Getting reports from 02/01/2017 00:00:00 to 02/28/2017 23:59:59\n",
      "[2025-11-30 14:54:44,069 INFO] Getting reports from 03/01/2017 00:00:00 to 03/31/2017 23:59:59\n",
      "[2025-11-30 14:55:29,938 INFO] Getting reports from 04/01/2017 00:00:00 to 04/30/2017 23:59:59\n",
      "[2025-11-30 14:55:56,774 INFO] Getting reports from 05/01/2017 00:00:00 to 05/31/2017 23:59:59\n",
      "[2025-11-30 14:56:55,019 INFO] Getting reports from 06/01/2017 00:00:00 to 06/30/2017 23:59:59\n",
      "[2025-11-30 14:57:40,324 INFO] Getting reports from 07/01/2017 00:00:00 to 07/31/2017 23:59:59\n",
      "[2025-11-30 14:58:25,745 INFO] Getting reports from 08/01/2017 00:00:00 to 08/31/2017 23:59:59\n",
      "[2025-11-30 14:59:07,570 INFO] Getting reports from 09/01/2017 00:00:00 to 09/30/2017 23:59:59\n",
      "[2025-11-30 14:59:47,863 INFO] Getting reports from 10/01/2017 00:00:00 to 10/31/2017 23:59:59\n",
      "[2025-11-30 15:00:22,492 INFO] Getting reports from 11/01/2017 00:00:00 to 11/30/2017 23:59:59\n",
      "[2025-11-30 15:01:01,250 INFO] Getting reports from 12/01/2017 00:00:00 to 12/31/2017 23:59:59\n",
      "[2025-11-30 15:02:17,674 INFO] Getting reports from 01/01/2018 00:00:00 to 01/31/2018 23:59:59\n",
      "[2025-11-30 15:03:02,065 INFO] Getting reports from 02/01/2018 00:00:00 to 02/28/2018 23:59:59\n",
      "[2025-11-30 15:03:34,055 INFO] Getting reports from 03/01/2018 00:00:00 to 03/31/2018 23:59:59\n",
      "[2025-11-30 15:04:06,413 INFO] Getting reports from 04/01/2018 00:00:00 to 04/30/2018 23:59:59\n",
      "[2025-11-30 15:04:37,292 INFO] Getting reports from 05/01/2018 00:00:00 to 05/31/2018 23:59:59\n",
      "[2025-11-30 15:05:56,898 INFO] Getting reports from 06/01/2018 00:00:00 to 06/30/2018 23:59:59\n",
      "[2025-11-30 15:06:46,952 INFO] Getting reports from 07/01/2018 00:00:00 to 07/31/2018 23:59:59\n",
      "[2025-11-30 15:07:29,999 INFO] Getting reports from 08/01/2018 00:00:00 to 08/31/2018 23:59:59\n",
      "[2025-11-30 15:08:39,870 INFO] Getting reports from 09/01/2018 00:00:00 to 09/30/2018 23:59:59\n",
      "[2025-11-30 15:09:16,054 INFO] Getting reports from 10/01/2018 00:00:00 to 10/31/2018 23:59:59\n",
      "[2025-11-30 15:10:02,128 INFO] Getting reports from 11/01/2018 00:00:00 to 11/30/2018 23:59:59\n",
      "[2025-11-30 15:10:43,528 INFO] Getting reports from 12/01/2018 00:00:00 to 12/31/2018 23:59:59\n",
      "[2025-11-30 15:11:09,568 INFO] Getting reports from 01/01/2019 00:00:00 to 01/31/2019 23:59:59\n",
      "[2025-11-30 15:11:42,693 INFO] Getting reports from 02/01/2019 00:00:00 to 02/28/2019 23:59:59\n",
      "[2025-11-30 15:12:10,968 INFO] Getting reports from 03/01/2019 00:00:00 to 03/31/2019 23:59:59\n",
      "[2025-11-30 15:12:45,701 INFO] Getting reports from 04/01/2019 00:00:00 to 04/30/2019 23:59:59\n",
      "[2025-11-30 15:13:37,441 INFO] Getting reports from 05/01/2019 00:00:00 to 05/31/2019 23:59:59\n",
      "[2025-11-30 15:15:03,281 INFO] Getting reports from 06/01/2019 00:00:00 to 06/30/2019 23:59:59\n",
      "[2025-11-30 15:15:36,818 INFO] Getting reports from 07/01/2019 00:00:00 to 07/31/2019 23:59:59\n",
      "[2025-11-30 15:16:07,891 INFO] Getting reports from 08/01/2019 00:00:00 to 08/31/2019 23:59:59\n",
      "[2025-11-30 15:17:07,765 INFO] Getting reports from 09/01/2019 00:00:00 to 09/30/2019 23:59:59\n",
      "[2025-11-30 15:17:33,645 INFO] Getting reports from 10/01/2019 00:00:00 to 10/31/2019 23:59:59\n",
      "[2025-11-30 15:18:06,762 INFO] Getting reports from 11/01/2019 00:00:00 to 11/30/2019 23:59:59\n",
      "[2025-11-30 15:18:39,167 INFO] Getting reports from 12/01/2019 00:00:00 to 12/31/2019 23:59:59\n",
      "[2025-11-30 15:19:20,999 INFO] Getting reports from 01/01/2020 00:00:00 to 01/31/2020 23:59:59\n",
      "[2025-11-30 15:19:53,315 INFO] Getting reports from 02/01/2020 00:00:00 to 02/29/2020 23:59:59\n",
      "[2025-11-30 15:20:47,263 INFO] Getting reports from 03/01/2020 00:00:00 to 03/31/2020 23:59:59\n",
      "[2025-11-30 15:21:34,693 INFO] Getting reports from 04/01/2020 00:00:00 to 04/30/2020 23:59:59\n",
      "[2025-11-30 15:22:06,373 INFO] Getting reports from 05/01/2020 00:00:00 to 05/31/2020 23:59:59\n",
      "[2025-11-30 15:23:07,143 INFO] Getting reports from 06/01/2020 00:00:00 to 06/30/2020 23:59:59\n",
      "[2025-11-30 15:23:27,168 INFO] Getting reports from 07/01/2020 00:00:00 to 07/31/2020 23:59:59\n",
      "[2025-11-30 15:23:51,969 INFO] Getting reports from 08/01/2020 00:00:00 to 08/31/2020 23:59:59\n",
      "[2025-11-30 15:24:48,594 INFO] Getting reports from 09/01/2020 00:00:00 to 09/30/2020 23:59:59\n",
      "[2025-11-30 15:25:18,215 INFO] Getting reports from 10/01/2020 00:00:00 to 10/31/2020 23:59:59\n",
      "[2025-11-30 15:25:39,841 INFO] Getting reports from 11/01/2020 00:00:00 to 11/30/2020 23:59:59\n",
      "[2025-11-30 15:26:02,132 INFO] Getting reports from 12/01/2020 00:00:00 to 12/31/2020 23:59:59\n",
      "[2025-11-30 15:26:35,959 INFO] Getting reports from 01/01/2021 00:00:00 to 01/31/2021 23:59:59\n",
      "[2025-11-30 15:27:04,896 INFO] Getting reports from 02/01/2021 00:00:00 to 02/28/2021 23:59:59\n",
      "[2025-11-30 15:27:27,369 INFO] Getting reports from 03/01/2021 00:00:00 to 03/31/2021 23:59:59\n",
      "[2025-11-30 15:27:50,401 INFO] Getting reports from 04/01/2021 00:00:00 to 04/30/2021 23:59:59\n",
      "[2025-11-30 15:28:27,740 INFO] Getting reports from 05/01/2021 00:00:00 to 05/31/2021 23:59:59\n",
      "[2025-11-30 15:29:26,013 INFO] Getting reports from 06/01/2021 00:00:00 to 06/30/2021 23:59:59\n",
      "[2025-11-30 15:29:46,765 INFO] Getting reports from 07/01/2021 00:00:00 to 07/31/2021 23:59:59\n",
      "[2025-11-30 15:30:32,609 INFO] Getting reports from 08/01/2021 00:00:00 to 08/31/2021 23:59:59\n",
      "[2025-11-30 15:31:10,057 INFO] Getting reports from 09/01/2021 00:00:00 to 09/30/2021 23:59:59\n",
      "[2025-11-30 15:31:33,733 INFO] Getting reports from 10/01/2021 00:00:00 to 10/31/2021 23:59:59\n",
      "[2025-11-30 15:31:52,862 INFO] Getting reports from 11/01/2021 00:00:00 to 11/30/2021 23:59:59\n",
      "[2025-11-30 15:32:23,936 INFO] Getting reports from 12/01/2021 00:00:00 to 12/31/2021 23:59:59\n",
      "[2025-11-30 15:32:49,210 INFO] Getting reports from 01/01/2022 00:00:00 to 01/31/2022 23:59:59\n",
      "[2025-11-30 15:33:26,740 INFO] Getting reports from 02/01/2022 00:00:00 to 02/28/2022 23:59:59\n",
      "[2025-11-30 15:33:51,953 INFO] Getting reports from 03/01/2022 00:00:00 to 03/31/2022 23:59:59\n",
      "[2025-11-30 15:34:25,722 INFO] Getting reports from 04/01/2022 00:00:00 to 04/30/2022 23:59:59\n",
      "[2025-11-30 15:34:52,448 INFO] Getting reports from 05/01/2022 00:00:00 to 05/31/2022 23:59:59\n",
      "[2025-11-30 15:35:30,847 INFO] Getting reports from 06/01/2022 00:00:00 to 06/30/2022 23:59:59\n",
      "[2025-11-30 15:35:52,293 INFO] Getting reports from 07/01/2022 00:00:00 to 07/31/2022 23:59:59\n",
      "[2025-11-30 15:36:19,253 INFO] Getting reports from 08/01/2022 00:00:00 to 08/31/2022 23:59:59\n",
      "[2025-11-30 15:36:39,663 INFO] Getting reports from 09/01/2022 00:00:00 to 09/30/2022 23:59:59\n",
      "[2025-11-30 15:36:57,542 INFO] Getting reports from 10/01/2022 00:00:00 to 10/31/2022 23:59:59\n",
      "[2025-11-30 15:37:13,094 INFO] Getting reports from 11/01/2022 00:00:00 to 11/30/2022 23:59:59\n",
      "[2025-11-30 15:37:39,733 INFO] Getting reports from 12/01/2022 00:00:00 to 12/31/2022 23:59:59\n",
      "[2025-11-30 15:38:08,396 INFO] Getting reports from 01/01/2023 00:00:00 to 01/31/2023 23:59:59\n",
      "[2025-11-30 15:38:38,463 INFO] Getting reports from 02/01/2023 00:00:00 to 02/28/2023 23:59:59\n",
      "[2025-11-30 15:39:01,196 INFO] Getting reports from 03/01/2023 00:00:00 to 03/31/2023 23:59:59\n",
      "[2025-11-30 15:39:22,934 INFO] Getting reports from 04/01/2023 00:00:00 to 04/30/2023 23:59:59\n",
      "[2025-11-30 15:39:43,378 INFO] Getting reports from 05/01/2023 00:00:00 to 05/31/2023 23:59:59\n",
      "[2025-11-30 15:40:18,930 INFO] Getting reports from 06/01/2023 00:00:00 to 06/30/2023 23:59:59\n",
      "[2025-11-30 15:40:44,058 INFO] Getting reports from 07/01/2023 00:00:00 to 07/31/2023 23:59:59\n",
      "[2025-11-30 15:40:59,024 INFO] Getting reports from 08/01/2023 00:00:00 to 08/31/2023 23:59:59\n",
      "[2025-11-30 15:41:16,953 INFO] Getting reports from 09/01/2023 00:00:00 to 09/30/2023 23:59:59\n",
      "[2025-11-30 15:41:53,805 INFO] Getting reports from 10/01/2023 00:00:00 to 10/31/2023 23:59:59\n",
      "[2025-11-30 15:42:27,723 INFO] Getting reports from 11/01/2023 00:00:00 to 11/30/2023 23:59:59\n",
      "[2025-11-30 15:42:55,963 INFO] Getting reports from 12/01/2023 00:00:00 to 12/31/2023 23:59:59\n",
      "[2025-11-30 15:43:31,810 INFO] Getting reports from 01/01/2024 00:00:00 to 01/31/2024 23:59:59\n",
      "[2025-11-30 15:44:12,790 INFO] Getting reports from 02/01/2024 00:00:00 to 02/29/2024 23:59:59\n",
      "[2025-11-30 15:44:33,188 INFO] Getting reports from 03/01/2024 00:00:00 to 03/31/2024 23:59:59\n",
      "[2025-11-30 15:44:57,786 INFO] Getting reports from 04/01/2024 00:00:00 to 04/30/2024 23:59:59\n",
      "[2025-11-30 15:45:23,673 INFO] Getting reports from 05/01/2024 00:00:00 to 05/31/2024 23:59:59\n",
      "[2025-11-30 15:45:56,669 INFO] Getting reports from 06/01/2024 00:00:00 to 06/30/2024 23:59:59\n",
      "[2025-11-30 15:46:25,634 INFO] Getting reports from 07/01/2024 00:00:00 to 07/31/2024 23:59:59\n",
      "[2025-11-30 15:46:51,284 INFO] Getting reports from 08/01/2024 00:00:00 to 08/31/2024 23:59:59\n",
      "[2025-11-30 15:47:27,484 INFO] Getting reports from 09/01/2024 00:00:00 to 09/30/2024 23:59:59\n",
      "[2025-11-30 15:47:55,480 INFO] Getting reports from 10/01/2024 00:00:00 to 10/31/2024 23:59:59\n",
      "[2025-11-30 15:48:14,505 INFO] Getting reports from 11/01/2024 00:00:00 to 11/30/2024 23:59:59\n",
      "[2025-11-30 15:48:44,636 INFO] Getting reports from 12/01/2024 00:00:00 to 12/31/2024 23:59:59\n",
      "[2025-11-30 15:49:11,372 INFO] Getting reports from 01/01/2025 00:00:00 to 01/31/2025 23:59:59\n",
      "[2025-11-30 15:49:38,071 INFO] Getting reports from 02/01/2025 00:00:00 to 02/28/2025 23:59:59\n",
      "[2025-11-30 15:50:09,211 INFO] Getting reports from 03/01/2025 00:00:00 to 03/31/2025 23:59:59\n",
      "[2025-11-30 15:50:49,434 INFO] Getting reports from 04/01/2025 00:00:00 to 04/30/2025 23:59:59\n",
      "[2025-11-30 15:51:22,101 INFO] Getting reports from 05/01/2025 00:00:00 to 05/31/2025 23:59:59\n",
      "[2025-11-30 15:51:54,720 INFO] Getting reports from 06/01/2025 00:00:00 to 06/30/2025 23:59:59\n",
      "[2025-11-30 15:52:28,251 INFO] Getting reports from 07/01/2025 00:00:00 to 07/31/2025 23:59:59\n",
      "[2025-11-30 15:53:04,936 INFO] Getting reports from 08/01/2025 00:00:00 to 08/31/2025 23:59:59\n",
      "[2025-11-30 15:54:05,125 INFO] Getting reports from 09/01/2025 00:00:00 to 09/30/2025 23:59:59\n",
      "[2025-11-30 15:54:35,389 INFO] Getting reports from 10/01/2025 00:00:00 to 10/31/2025 23:59:59\n",
      "[2025-11-30 15:55:13,279 INFO] Getting reports from 11/01/2025 00:00:00 to 11/30/2025 23:59:59\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:47:45.906721Z",
     "start_time": "2025-11-30T12:46:32.557686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Notebook Cell ---\n",
    "\"\"\" Test scraping first N Senator reports (including PDFs) \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "ROOT = 'https://efdsearch.senate.gov'\n",
    "LANDING_PAGE_URL = f'{ROOT}/search/home/'\n",
    "SEARCH_PAGE_URL = f'{ROOT}/search/'\n",
    "REPORTS_URL = f'{ROOT}/search/report/data/'\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "RATE_LIMIT_SECS = 2\n",
    "PDF_PREFIX = '/search/view/paper/'\n",
    "\n",
    "REPORT_COL_NAMES = [\n",
    "    'tx_date',\n",
    "    'file_date',\n",
    "    'last_name',\n",
    "    'first_name',\n",
    "    'order_type',\n",
    "    'ticker',\n",
    "    'asset_name',\n",
    "    'tx_amount',\n",
    "    'link'\n",
    "]\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s %(levelname)s] %(message)s')\n",
    "LOGGER = logging.getLogger()\n",
    "\n",
    "def add_rate_limit(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        time.sleep(RATE_LIMIT_SECS)\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "def _csrf(client: requests.Session) -> str:\n",
    "    landing_page_response = client.get(LANDING_PAGE_URL)\n",
    "    landing_page = BeautifulSoup(landing_page_response.text, \"html.parser\")\n",
    "    form_csrf = landing_page.find(attrs={'name': 'csrfmiddlewaretoken'})['value']\n",
    "    client.post(LANDING_PAGE_URL,\n",
    "                data={'csrfmiddlewaretoken': form_csrf, 'prohibition_agreement': '1'},\n",
    "                headers={'Referer': LANDING_PAGE_URL})\n",
    "    return client.cookies.get('csrftoken') or client.cookies.get('csrf')\n",
    "\n",
    "def reports_api(client: requests.Session, offset: int, token: str):\n",
    "    data = {\n",
    "        'start': str(offset),\n",
    "        'length': str(BATCH_SIZE),\n",
    "        'report_types': '[11]',\n",
    "        'filer_types': '[]',\n",
    "        'submitted_start_date': '01/01/2012 00:00:00',\n",
    "        'submitted_end_date': '',\n",
    "        'candidate_state': '',\n",
    "        'senator_state': '',\n",
    "        'office_id': '',\n",
    "        'first_name': '',\n",
    "        'last_name': '',\n",
    "        'csrfmiddlewaretoken': token\n",
    "    }\n",
    "    LOGGER.info(f'Getting rows starting at {offset}')\n",
    "    resp = client.post(REPORTS_URL, data=data, headers={'Referer': SEARCH_PAGE_URL})\n",
    "    return resp.json()['data']\n",
    "\n",
    "def senator_reports(client: requests.Session):\n",
    "    token = _csrf(client)\n",
    "    idx = 0\n",
    "    all_reports = []\n",
    "    while True:\n",
    "        batch = reports_api(client, idx, token)\n",
    "        if not batch:\n",
    "            break\n",
    "        all_reports.extend(batch)\n",
    "        idx += BATCH_SIZE\n",
    "    return all_reports\n",
    "\n",
    "def _tbody_from_link(client: requests.Session, link: str):\n",
    "    report_url = f'{ROOT}{link}'\n",
    "    resp = client.get(report_url)\n",
    "    report = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    tbodies = report.find_all('tbody')\n",
    "    return tbodies[0] if tbodies else None\n",
    "\n",
    "def txs_for_report_all(client: requests.Session, row):\n",
    "    first, last, _, link_html, date_received = row\n",
    "    link_soup = BeautifulSoup(link_html, \"html.parser\")\n",
    "    a_tag = link_soup.a\n",
    "    link = a_tag.get('href') if a_tag else None\n",
    "    full_link = f\"{ROOT}{link}\" if link else None\n",
    "\n",
    "    if not link or link.startswith(PDF_PREFIX):\n",
    "        return pd.DataFrame([{\n",
    "            'tx_date': None,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': None,\n",
    "            'ticker': None,\n",
    "            'asset_name': None,\n",
    "            'tx_amount': None,\n",
    "            'link': full_link\n",
    "        }])\n",
    "\n",
    "    tbody = _tbody_from_link(client, link)\n",
    "    if not tbody:\n",
    "        return pd.DataFrame([{\n",
    "            'tx_date': None,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': None,\n",
    "            'ticker': None,\n",
    "            'asset_name': None,\n",
    "            'tx_amount': None,\n",
    "            'link': full_link\n",
    "        }])\n",
    "\n",
    "    stocks = []\n",
    "    for tr in tbody.find_all('tr'):\n",
    "        cols = [c.get_text().strip() for c in tr.find_all('td')]\n",
    "        if len(cols) < 8:\n",
    "            continue\n",
    "        tx_date, ticker, asset_name, asset_type, order_type, tx_amount = \\\n",
    "            cols[1], cols[3], cols[4], cols[5], cols[6], cols[7]\n",
    "        if asset_type != 'Stock' and ticker.strip() in ('--', ''):\n",
    "            continue\n",
    "        stocks.append({\n",
    "            'tx_date': tx_date,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': order_type,\n",
    "            'ticker': ticker,\n",
    "            'asset_name': asset_name,\n",
    "            'tx_amount': tx_amount,\n",
    "            'link': full_link\n",
    "        })\n",
    "    return pd.DataFrame(stocks)\n",
    "\n",
    "# --- Test cell: nur die ersten 5 Reports ---\n",
    "client = requests.Session()\n",
    "client.get = add_rate_limit(client.get)\n",
    "client.post = add_rate_limit(client.post)\n",
    "\n",
    "reports = senator_reports(client)[:5]  # nur erste 5 Reports\n",
    "all_txs = pd.concat([txs_for_report_all(client, r) for r in reports], ignore_index=True)\n",
    "\n",
    "# CSV Export\n",
    "all_txs.to_csv('senator_transactions_test.csv', index=False)\n",
    "\n",
    "# Vorschau\n",
    "all_txs.head()\n"
   ],
   "id": "fb156c912fb86673",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-30 13:46:37,476 INFO] Getting rows starting at 0\n",
      "[2025-11-30 13:46:39,858 INFO] Getting rows starting at 100\n",
      "[2025-11-30 13:46:42,197 INFO] Getting rows starting at 200\n",
      "[2025-11-30 13:46:44,559 INFO] Getting rows starting at 300\n",
      "[2025-11-30 13:46:46,927 INFO] Getting rows starting at 400\n",
      "[2025-11-30 13:46:49,293 INFO] Getting rows starting at 500\n",
      "[2025-11-30 13:46:51,646 INFO] Getting rows starting at 600\n",
      "[2025-11-30 13:46:53,978 INFO] Getting rows starting at 700\n",
      "[2025-11-30 13:46:56,550 INFO] Getting rows starting at 800\n",
      "[2025-11-30 13:46:58,906 INFO] Getting rows starting at 900\n",
      "[2025-11-30 13:47:01,248 INFO] Getting rows starting at 1000\n",
      "[2025-11-30 13:47:03,590 INFO] Getting rows starting at 1100\n",
      "[2025-11-30 13:47:05,925 INFO] Getting rows starting at 1200\n",
      "[2025-11-30 13:47:08,321 INFO] Getting rows starting at 1300\n",
      "[2025-11-30 13:47:10,756 INFO] Getting rows starting at 1400\n",
      "[2025-11-30 13:47:13,141 INFO] Getting rows starting at 1500\n",
      "[2025-11-30 13:47:15,519 INFO] Getting rows starting at 1600\n",
      "[2025-11-30 13:47:17,922 INFO] Getting rows starting at 1700\n",
      "[2025-11-30 13:47:20,251 INFO] Getting rows starting at 1800\n",
      "[2025-11-30 13:47:22,661 INFO] Getting rows starting at 1900\n",
      "[2025-11-30 13:47:25,016 INFO] Getting rows starting at 2000\n",
      "[2025-11-30 13:47:27,385 INFO] Getting rows starting at 2100\n",
      "[2025-11-30 13:47:29,719 INFO] Getting rows starting at 2200\n",
      "[2025-11-30 13:47:32,068 INFO] Getting rows starting at 2300\n",
      "[2025-11-30 13:47:34,371 INFO] Getting rows starting at 2400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      tx_date   file_date  last_name first_name   order_type ticker  \\\n",
       "0  11/25/2025  11/27/2025  McCormick    David H     Purchase   BITB   \n",
       "1  11/24/2025  11/27/2025  McCormick    David H     Purchase   BITB   \n",
       "2  11/21/2025  11/26/2025      Smith       Tina  Sale (Full)   HBAN   \n",
       "3  07/10/2025  11/24/2025     Peters     Gary C  Sale (Full)    OGN   \n",
       "4  11/03/2025  11/21/2025     Mullin  Markwayne  Sale (Full)     FI   \n",
       "\n",
       "                                          asset_name            tx_amount  \\\n",
       "0                                Bitwise Bitcoin ETF    $15,001 - $50,000   \n",
       "1                                Bitwise Bitcoin ETF   $50,001 - $100,000   \n",
       "2  Huntington Bancshares Incorporated - Common Stock  $100,001 - $250,000   \n",
       "3                                      Organon & Co.     $1,001 - $15,000   \n",
       "4                                         Fiserv Inc    $15,001 - $50,000   \n",
       "\n",
       "                                                link  \n",
       "0  https://efdsearch.senate.gov/search/view/ptr/5...  \n",
       "1  https://efdsearch.senate.gov/search/view/ptr/5...  \n",
       "2  https://efdsearch.senate.gov/search/view/ptr/8...  \n",
       "3  https://efdsearch.senate.gov/search/view/ptr/1...  \n",
       "4  https://efdsearch.senate.gov/search/view/ptr/8...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tx_date</th>\n",
       "      <th>file_date</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>order_type</th>\n",
       "      <th>ticker</th>\n",
       "      <th>asset_name</th>\n",
       "      <th>tx_amount</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/25/2025</td>\n",
       "      <td>11/27/2025</td>\n",
       "      <td>McCormick</td>\n",
       "      <td>David H</td>\n",
       "      <td>Purchase</td>\n",
       "      <td>BITB</td>\n",
       "      <td>Bitwise Bitcoin ETF</td>\n",
       "      <td>$15,001 - $50,000</td>\n",
       "      <td>https://efdsearch.senate.gov/search/view/ptr/5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/24/2025</td>\n",
       "      <td>11/27/2025</td>\n",
       "      <td>McCormick</td>\n",
       "      <td>David H</td>\n",
       "      <td>Purchase</td>\n",
       "      <td>BITB</td>\n",
       "      <td>Bitwise Bitcoin ETF</td>\n",
       "      <td>$50,001 - $100,000</td>\n",
       "      <td>https://efdsearch.senate.gov/search/view/ptr/5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/21/2025</td>\n",
       "      <td>11/26/2025</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Tina</td>\n",
       "      <td>Sale (Full)</td>\n",
       "      <td>HBAN</td>\n",
       "      <td>Huntington Bancshares Incorporated - Common Stock</td>\n",
       "      <td>$100,001 - $250,000</td>\n",
       "      <td>https://efdsearch.senate.gov/search/view/ptr/8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07/10/2025</td>\n",
       "      <td>11/24/2025</td>\n",
       "      <td>Peters</td>\n",
       "      <td>Gary C</td>\n",
       "      <td>Sale (Full)</td>\n",
       "      <td>OGN</td>\n",
       "      <td>Organon &amp; Co.</td>\n",
       "      <td>$1,001 - $15,000</td>\n",
       "      <td>https://efdsearch.senate.gov/search/view/ptr/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/03/2025</td>\n",
       "      <td>11/21/2025</td>\n",
       "      <td>Mullin</td>\n",
       "      <td>Markwayne</td>\n",
       "      <td>Sale (Full)</td>\n",
       "      <td>FI</td>\n",
       "      <td>Fiserv Inc</td>\n",
       "      <td>$15,001 - $50,000</td>\n",
       "      <td>https://efdsearch.senate.gov/search/view/ptr/8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
