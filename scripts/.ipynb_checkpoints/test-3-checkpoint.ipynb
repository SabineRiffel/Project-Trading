{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b3c8d5842cb70d",
   "metadata": {},
   "source": [
    "// Scraper zum ziehen der transaktionen aus der webseite// (dauert ca 1 h:20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fec415edb9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Scrape stock transactions from Senator periodic filings (resumable + ETA logging) \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "ROOT = 'https://efdsearch.senate.gov'\n",
    "LANDING_PAGE_URL = f'{ROOT}/search/home/'\n",
    "SEARCH_PAGE_URL = f'{ROOT}/search/'\n",
    "REPORTS_URL = f'{ROOT}/search/report/data/'\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "RATE_LIMIT_SECS = 2\n",
    "PDF_PREFIX = '/search/view/paper/'\n",
    "OUTPUT_CSV = 'senator_transactions_all.csv'\n",
    "MAX_RETRIES = 3\n",
    "TIMEOUT = 10\n",
    "\n",
    "REPORT_COL_NAMES = [\n",
    "    'tx_date',\n",
    "    'file_date',\n",
    "    'last_name',\n",
    "    'first_name',\n",
    "    'order_type',\n",
    "    'ticker',\n",
    "    'asset_name',\n",
    "    'tx_amount',\n",
    "    'link'\n",
    "]\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "def add_rate_limit(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        time.sleep(RATE_LIMIT_SECS)\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "def _csrf(client: requests.Session) -> str:\n",
    "    landing_page_response = client.get(LANDING_PAGE_URL)\n",
    "    assert landing_page_response.url == LANDING_PAGE_URL, \"Failed to fetch filings landing page\"\n",
    "    landing_page = BeautifulSoup(landing_page_response.text, \"html.parser\")\n",
    "    form_csrf = landing_page.find(attrs={'name': 'csrfmiddlewaretoken'})['value']\n",
    "\n",
    "    client.post(LANDING_PAGE_URL,\n",
    "                data={'csrfmiddlewaretoken': form_csrf, 'prohibition_agreement': '1'},\n",
    "                headers={'Referer': LANDING_PAGE_URL})\n",
    "\n",
    "    return client.cookies.get('csrftoken') or client.cookies.get('csrf')\n",
    "\n",
    "def reports_api(client: requests.Session, start_date: str, end_date: str, token: str):\n",
    "    data = {\n",
    "        'start': '0',  # immer vom Anfang\n",
    "        'length': str(BATCH_SIZE),\n",
    "        'report_types': '[11]',\n",
    "        'filer_types': '[]',\n",
    "        'submitted_start_date': start_date,\n",
    "        'submitted_end_date': end_date,\n",
    "        'candidate_state': '',\n",
    "        'senator_state': '',\n",
    "        'office_id': '',\n",
    "        'first_name': '',\n",
    "        'last_name': '',\n",
    "        'csrfmiddlewaretoken': token\n",
    "    }\n",
    "    LOGGER.info(f'Getting reports from {start_date} to {end_date}')\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            resp = client.post(REPORTS_URL, data=data, headers={'Referer': SEARCH_PAGE_URL}, timeout=TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()['data']\n",
    "        except Exception as e:\n",
    "            LOGGER.warning(f'Attempt {attempt+1} failed: {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    raise RuntimeError(f'Failed to fetch reports from {start_date} to {end_date} after {MAX_RETRIES} attempts')\n",
    "\n",
    "def _tbody_from_link(client: requests.Session, link: str):\n",
    "    report_url = f'{ROOT}{link}'\n",
    "    resp = client.get(report_url)\n",
    "    if resp.url == LANDING_PAGE_URL:\n",
    "        _csrf(client)\n",
    "        resp = client.get(report_url)\n",
    "    report = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    tbodies = report.find_all('tbody')\n",
    "    return tbodies[0] if tbodies else None\n",
    "\n",
    "def txs_for_report_all(client: requests.Session, row):\n",
    "    first, last, _, link_html, date_received = row\n",
    "    link_soup = BeautifulSoup(link_html, \"html.parser\")\n",
    "    a_tag = link_soup.a\n",
    "    link = a_tag.get('href') if a_tag else None\n",
    "\n",
    "    if not link or link.startswith(PDF_PREFIX):\n",
    "        return pd.DataFrame([{\n",
    "            'tx_date': None,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': None,\n",
    "            'ticker': None,\n",
    "            'asset_name': None,\n",
    "            'tx_amount': None,\n",
    "            'link': f\"{ROOT}{link}\" if link else None\n",
    "        }])\n",
    "\n",
    "    tbody = _tbody_from_link(client, link)\n",
    "    if not tbody:\n",
    "        return pd.DataFrame([{\n",
    "            'tx_date': None,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': None,\n",
    "            'ticker': None,\n",
    "            'asset_name': None,\n",
    "            'tx_amount': None,\n",
    "            'link': f\"{ROOT}{link}\"\n",
    "        }])\n",
    "\n",
    "    stocks = []\n",
    "    for tr in tbody.find_all('tr'):\n",
    "        cols = [c.get_text().strip() for c in tr.find_all('td')]\n",
    "        if len(cols) < 8:\n",
    "            continue\n",
    "        tx_date, ticker, asset_name, asset_type, order_type, tx_amount = \\\n",
    "            cols[1], cols[3], cols[4], cols[5], cols[6], cols[7]\n",
    "        if asset_type != 'Stock' and ticker.strip() in ('--', ''):\n",
    "            continue\n",
    "        stocks.append({\n",
    "            'tx_date': tx_date,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': order_type,\n",
    "            'ticker': ticker,\n",
    "            'asset_name': asset_name,\n",
    "            'tx_amount': tx_amount,\n",
    "            'link': f\"{ROOT}{link}\"\n",
    "        })\n",
    "    return pd.DataFrame(stocks)\n",
    "\n",
    "def main():\n",
    "    LOGGER.info('Initializing client')\n",
    "    client = requests.Session()\n",
    "    client.get = add_rate_limit(client.get)\n",
    "    client.post = add_rate_limit(client.post)\n",
    "\n",
    "    token = _csrf(client)\n",
    "\n",
    "    start = datetime(2012, 1, 1)\n",
    "    end = datetime.today()\n",
    "\n",
    "    # CSV vorbereiten\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        mode = 'a'\n",
    "        header = False\n",
    "    else:\n",
    "        mode = 'w'\n",
    "        header = True\n",
    "\n",
    "    while start < end:\n",
    "        month_end = (start.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(days=1)\n",
    "        start_str = start.strftime(\"%m/%d/%Y 00:00:00\")\n",
    "        end_str = month_end.strftime(\"%m/%d/%Y 23:59:59\")\n",
    "\n",
    "        batch = reports_api(client, start_str, end_str, token)\n",
    "\n",
    "        for r in batch:\n",
    "            df = txs_for_report_all(client, r)\n",
    "            df.to_csv(OUTPUT_CSV, mode=mode, header=header, index=False)\n",
    "            header = False\n",
    "            mode = 'a'\n",
    "\n",
    "        start = month_end + timedelta(days=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='[%(asctime)s %(levelname)s] %(message)s')\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fbc7f48056038",
   "metadata": {},
   "source": [
    "TEST für ABOVE für 5 Einträge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb156c912fb86673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook Cell ---\n",
    "\"\"\" Test scraping first N Senator reports (including PDFs) \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "ROOT = 'https://efdsearch.senate.gov'\n",
    "LANDING_PAGE_URL = f'{ROOT}/search/home/'\n",
    "SEARCH_PAGE_URL = f'{ROOT}/search/'\n",
    "REPORTS_URL = f'{ROOT}/search/report/data/'\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "RATE_LIMIT_SECS = 2\n",
    "PDF_PREFIX = '/search/view/paper/'\n",
    "\n",
    "REPORT_COL_NAMES = [\n",
    "    'tx_date',\n",
    "    'file_date',\n",
    "    'last_name',\n",
    "    'first_name',\n",
    "    'order_type',\n",
    "    'ticker',\n",
    "    'asset_name',\n",
    "    'tx_amount',\n",
    "    'link'\n",
    "]\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s %(levelname)s] %(message)s')\n",
    "LOGGER = logging.getLogger()\n",
    "\n",
    "def add_rate_limit(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        time.sleep(RATE_LIMIT_SECS)\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "def _csrf(client: requests.Session) -> str:\n",
    "    landing_page_response = client.get(LANDING_PAGE_URL)\n",
    "    landing_page = BeautifulSoup(landing_page_response.text, \"html.parser\")\n",
    "    form_csrf = landing_page.find(attrs={'name': 'csrfmiddlewaretoken'})['value']\n",
    "    client.post(LANDING_PAGE_URL,\n",
    "                data={'csrfmiddlewaretoken': form_csrf, 'prohibition_agreement': '1'},\n",
    "                headers={'Referer': LANDING_PAGE_URL})\n",
    "    return client.cookies.get('csrftoken') or client.cookies.get('csrf')\n",
    "\n",
    "def reports_api(client: requests.Session, offset: int, token: str):\n",
    "    data = {\n",
    "        'start': str(offset),\n",
    "        'length': str(BATCH_SIZE),\n",
    "        'report_types': '[11]',\n",
    "        'filer_types': '[]',\n",
    "        'submitted_start_date': '01/01/2012 00:00:00',\n",
    "        'submitted_end_date': '',\n",
    "        'candidate_state': '',\n",
    "        'senator_state': '',\n",
    "        'office_id': '',\n",
    "        'first_name': '',\n",
    "        'last_name': '',\n",
    "        'csrfmiddlewaretoken': token\n",
    "    }\n",
    "    LOGGER.info(f'Getting rows starting at {offset}')\n",
    "    resp = client.post(REPORTS_URL, data=data, headers={'Referer': SEARCH_PAGE_URL})\n",
    "    return resp.json()['data']\n",
    "\n",
    "def senator_reports(client: requests.Session):\n",
    "    token = _csrf(client)\n",
    "    idx = 0\n",
    "    all_reports = []\n",
    "    while True:\n",
    "        batch = reports_api(client, idx, token)\n",
    "        if not batch:\n",
    "            break\n",
    "        all_reports.extend(batch)\n",
    "        idx += BATCH_SIZE\n",
    "    return all_reports\n",
    "\n",
    "def _tbody_from_link(client: requests.Session, link: str):\n",
    "    report_url = f'{ROOT}{link}'\n",
    "    resp = client.get(report_url)\n",
    "    report = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    tbodies = report.find_all('tbody')\n",
    "    return tbodies[0] if tbodies else None\n",
    "\n",
    "def txs_for_report_all(client: requests.Session, row):\n",
    "    first, last, _, link_html, date_received = row\n",
    "    link_soup = BeautifulSoup(link_html, \"html.parser\")\n",
    "    a_tag = link_soup.a\n",
    "    link = a_tag.get('href') if a_tag else None\n",
    "    full_link = f\"{ROOT}{link}\" if link else None\n",
    "\n",
    "    if not link or link.startswith(PDF_PREFIX):\n",
    "        return pd.DataFrame([{\n",
    "            'tx_date': None,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': None,\n",
    "            'ticker': None,\n",
    "            'asset_name': None,\n",
    "            'tx_amount': None,\n",
    "            'link': full_link\n",
    "        }])\n",
    "\n",
    "    tbody = _tbody_from_link(client, link)\n",
    "    if not tbody:\n",
    "        return pd.DataFrame([{\n",
    "            'tx_date': None,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': None,\n",
    "            'ticker': None,\n",
    "            'asset_name': None,\n",
    "            'tx_amount': None,\n",
    "            'link': full_link\n",
    "        }])\n",
    "\n",
    "    stocks = []\n",
    "    for tr in tbody.find_all('tr'):\n",
    "        cols = [c.get_text().strip() for c in tr.find_all('td')]\n",
    "        if len(cols) < 8:\n",
    "            continue\n",
    "        tx_date, ticker, asset_name, asset_type, order_type, tx_amount = \\\n",
    "            cols[1], cols[3], cols[4], cols[5], cols[6], cols[7]\n",
    "        if asset_type != 'Stock' and ticker.strip() in ('--', ''):\n",
    "            continue\n",
    "        stocks.append({\n",
    "            'tx_date': tx_date,\n",
    "            'file_date': date_received,\n",
    "            'last_name': last,\n",
    "            'first_name': first,\n",
    "            'order_type': order_type,\n",
    "            'ticker': ticker,\n",
    "            'asset_name': asset_name,\n",
    "            'tx_amount': tx_amount,\n",
    "            'link': full_link\n",
    "        })\n",
    "    return pd.DataFrame(stocks)\n",
    "\n",
    "# --- Test cell: nur die ersten 5 Reports ---\n",
    "client = requests.Session()\n",
    "client.get = add_rate_limit(client.get)\n",
    "client.post = add_rate_limit(client.post)\n",
    "\n",
    "reports = senator_reports(client)[:5]  # nur erste 5 Reports\n",
    "all_txs = pd.concat([txs_for_report_all(client, r) for r in reports], ignore_index=True)\n",
    "\n",
    "# CSV Export\n",
    "all_txs.to_csv('senator_transactions_test.csv', index=False)\n",
    "\n",
    "# Vorschau\n",
    "all_txs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be95b580c80d052",
   "metadata": {},
   "source": [
    "Pie Chart zeigt wieviele % der Transactions als PDF einegreicht wurden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f32663b3ba771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CSV laden\n",
    "df = pd.read_csv('senator_transactions_all.csv', parse_dates=['tx_date', 'file_date'])\n",
    "\n",
    "# Filter auf 2022 bis heute und direkt eine Kopie erstellen\n",
    "df_filtered = df[df['file_date'] >= '2022-01-01'].copy()\n",
    "\n",
    "# Spalte is_pdf erstellen\n",
    "df_filtered['is_pdf'] = df_filtered['order_type'].isna()\n",
    "\n",
    "# Counts für Pie Chart\n",
    "counts = df_filtered['is_pdf'].value_counts()\n",
    "\n",
    "# Pie Chart erstellen\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(\n",
    "    [counts[False], counts[True]],\n",
    "    labels=['Transactions', 'PDFs'],\n",
    "    colors=['green', 'red'],\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90\n",
    ")\n",
    "plt.title('Transactions vs PDFs 2022-2025')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd92edca3957c7",
   "metadata": {},
   "source": [
    "Plot 1 zeigt Trades ingaseamt mit Welchen senatorenam meisten traden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542f57dbb46eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CSV laden\n",
    "df = pd.read_csv('senator_transactions_all.csv', parse_dates=['tx_date', 'file_date'])\n",
    "\n",
    "# Jahr extrahieren\n",
    "df['year'] = df['file_date'].dt.year\n",
    "\n",
    "# Filter 2022-2025\n",
    "df_filtered = df[(df['year'] >= 2022) & (df['year'] <= 2025)]\n",
    "\n",
    "# Farben für Top-Senatoren\n",
    "colors = ['#FF6F61', '#6B5B95', '#88B04B', '#F7CAC9', '#92A8D1']  # max 5 Top-Senatoren\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "years = sorted(df_filtered['year'].unique())\n",
    "total_trades_per_year = df_filtered.groupby('year').size()\n",
    "\n",
    "# x-Positionen als ganze Zahlen\n",
    "x_pos = range(len(years))\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    df_year = df_filtered[df_filtered['year'] == year]\n",
    "    top_senators = df_year['last_name'].value_counts().head(5)\n",
    "    top_sum = top_senators.sum()\n",
    "\n",
    "    # Graue Basis für Gesamt\n",
    "    plt.bar(x_pos[i], total_trades_per_year[year], color='lightgrey', label='Other Senators/PDFs' if i==0 else \"\")\n",
    "\n",
    "    bottom = 0\n",
    "    # Gestapelte farbige Bars für Top-Senatoren\n",
    "    for j, (senator, count) in enumerate(top_senators.items()):\n",
    "        plt.bar(x_pos[i], count, bottom=bottom, color=colors[j], label=senator if i==0 else \"\")\n",
    "        bottom += count\n",
    "\n",
    "plt.xticks(x_pos, years)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('amount of trades')\n",
    "plt.title('Top-Senators by Trades & PDFs per year (2022-2025)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66cf3225fa871",
   "metadata": {},
   "source": [
    "Plot 2 für trades je nach Größe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33b82a8465e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('senator_transactions_all.csv', parse_dates=['tx_date', 'file_date'])\n",
    "\n",
    "# Filter for 2022–2025\n",
    "df_filtered = df[df['file_date'].dt.year.between(2022, 2025)]\n",
    "\n",
    "# Define the desired order\n",
    "amount_order = [\n",
    "    \"$1,001 - $15,000\",\n",
    "    \"$15,001 - $50,000\",\n",
    "    \"$50,001 - $100,000\",\n",
    "    \"$100,001 - $250,000\",\n",
    "    \"$250,001 - $500,000\",\n",
    "    \"$500,001 - $1,000,000\",\n",
    "    \"Over $1,000,000\"\n",
    "]\n",
    "\n",
    "# Count trades per amount range\n",
    "amount_counts = df_filtered['tx_amount'].value_counts().reindex(amount_order, fill_value=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "amount_counts.plot(kind='bar', color='skyblue')\n",
    "plt.ylabel('Number of Trades')\n",
    "plt.xlabel('Trade Amount Range')\n",
    "plt.title('Number of Trades per Reported Amount Range (2022–2025)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452d2d8cc909620",
   "metadata": {},
   "source": [
    "PLOT 3 für total trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7f8ca2ccffb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('senator_transactions_all.csv', parse_dates=['file_date'])\n",
    "\n",
    "# Filter 2022–2025 and non-PDF trades\n",
    "df_filtered = df[\n",
    "    (df['file_date'].dt.year.between(2022, 2025)) &\n",
    "    (df['order_type'].notna())\n",
    "]\n",
    "\n",
    "total_trades = len(df_filtered)\n",
    "print(f\"Total non-PDF trades from 2022 to 2025: {total_trades}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9fa2056d0096e5",
   "metadata": {},
   "source": [
    "Gruppieren nach Tradegröße (Nur größer als 15K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4beb7542ada41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# CSV laden\n",
    "df = pd.read_csv('senator_transactions_all.csv', parse_dates=['file_date'])\n",
    "\n",
    "# Nur echte Trades, keine PDFs\n",
    "df_trades = df[df['order_type'].notna()].copy()\n",
    "\n",
    "# Hilfsfunktion, um tx_amount in eine Zahl zu konvertieren (wir nehmen immer den unteren Wert der Spanne)\n",
    "def parse_amount(val):\n",
    "    if pd.isna(val):\n",
    "        return 0\n",
    "    # Entferne $ und Kommas\n",
    "    val = val.replace('$','').replace(',','')\n",
    "    # Wenn Bereich, nimm den unteren Wert\n",
    "    if '-' in val:\n",
    "        return float(val.split('-')[0])\n",
    "    # Wenn \"Over 50,000,000\" -> nimm Zahl\n",
    "    if 'Over' in val:\n",
    "        return float(re.sub(r'\\D', '', val))\n",
    "    return float(val)\n",
    "\n",
    "df_trades['tx_amount_num'] = df_trades['tx_amount'].apply(parse_amount)\n",
    "\n",
    "# Filter Trades über 15k\n",
    "df_over_15k = df_trades[df_trades['tx_amount_num'] > 15000]\n",
    "\n",
    "# Speichern in neue CSV\n",
    "df_over_15k.to_csv('senator_trades_over_15k.csv', index=False)\n",
    "print(f\"Saved {len(df_over_15k)} trades over 15k to senator_trades_over_15k.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a630637677aba7",
   "metadata": {},
   "source": [
    "Herausfinden welche Trades keinen Ticker haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7034c07388271660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load trades\n",
    "df = pd.read_csv(\"senator_trades_over_15k.csv\")\n",
    "\n",
    "# Normalize ticker column\n",
    "df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip()\n",
    "\n",
    "# Define missing conditions\n",
    "missing_mask = (\n",
    "    df[\"ticker\"].isna() |\n",
    "    (df[\"ticker\"] == \"\") |\n",
    "    (df[\"ticker\"] == \"--\") |\n",
    "    (df[\"ticker\"].str.upper() == \"NAN\")\n",
    ")\n",
    "\n",
    "# Extract rows with missing tickers\n",
    "df_missing = df[missing_mask].copy()\n",
    "\n",
    "# Save\n",
    "df_missing.to_csv(\"trades_missing_ticker.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(df_missing)} rows with missing tickers → trades_missing_ticker.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9acddf2398d1c4",
   "metadata": {},
   "source": [
    "Gefiltert für 15k < trades ohne die NULL werte im ticker (Minus 250 einträge aber manuelle einträge von relevanten Apple Trades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465f77966e3260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ CSV laden\n",
    "df = pd.read_csv(\"senator_trades_over_15k.csv\", parse_dates=['tx_date', 'file_date'])\n",
    "\n",
    "# 2️⃣ Filter: keine Exchange Trades und gültige Ticker\n",
    "df_filtered = df[(df['order_type'].str.upper() != 'EXCHANGE') & (df['ticker'] != '--')]\n",
    "\n",
    "# 3️⃣ Speichern der allgemeinen Version\n",
    "df_filtered.to_csv(\"trades_over_15k_filtered.csv\", index=False)\n",
    "print(f\"Saved {len(df_filtered)} trades to trades_over_15k_filtered.csv\")\n",
    "\n",
    "# 4️⃣ Filter zusätzlich auf 2022 bis 2025\n",
    "df_filtered_2022_2025 = df_filtered[(df_filtered['tx_date'].dt.year >= 2022) & (df_filtered['tx_date'].dt.year <= 2025)]\n",
    "\n",
    "# 5️⃣ Speichern der 2022-2025 Version\n",
    "df_filtered_2022_2025.to_csv(\"trades_over_15k_filtered_2022_2025.csv\", index=False)\n",
    "print(f\"Saved {len(df_filtered_2022_2025)} trades to trades_over_15k_filtered_2022_2025.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0ba7e5bf593bf",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Load filtered trades\n",
    "df = pd.read_csv(\"trades_over_15k_filtered_2022_2025.csv\", parse_dates=['tx_date', 'file_date'])\n",
    "\n",
    "# 2️⃣ Filter Apple trades\n",
    "df_aapl = df[df['ticker'].str.upper() == 'AAPL'].copy()\n",
    "\n",
    "# 3️⃣ Load Apple stock price data\n",
    "aapl_prices = pd.read_parquet(\"data/AAPL.parquet\")\n",
    "# Datum konvertieren (UTC ignorieren)\n",
    "aapl_prices['date_only'] = pd.to_datetime(aapl_prices['timestamp']).dt.date\n",
    "\n",
    "# 4️⃣ Compare trade date to stock movement\n",
    "df_aapl['tx_date_only'] = df_aapl['tx_date'].dt.date\n",
    "df_aapl = df_aapl.merge(\n",
    "    aapl_prices[['date_only', 'open', 'close']],\n",
    "    left_on='tx_date_only',\n",
    "    right_on='date_only',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_aapl['movement'] = df_aapl.apply(lambda row: 'Up' if row['close'] > row['open'] else 'Down', axis=1)\n",
    "\n",
    "# 5️⃣ Save result\n",
    "df_aapl.to_csv(\"apple_trades_with_movement.csv\", index=False)\n",
    "\n",
    "# 6️⃣ Quick check\n",
    "df_aapl[['tx_date', 'order_type', 'tx_amount', 'open', 'close', 'movement']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd33aeb84915ef2",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "-!pip install notebook jupyterlab ipykernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31239e0b98eb39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mit fastparquet, falls PyArrow Probleme macht\n",
    "df = pd.read_parquet(\"data/AAPL.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "# Die ersten 5 Zeilen anzeigen\n",
    "print(df.head())\n",
    "\n",
    "# Optional: alle Spalten und Infos\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679bb4f-29e0-41b1-b2e8-cc362f6cc398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aapl_minute = pd.read_csv(\"data/AAPL.csv\")\n",
    "aapl_minute['timestamp'] = pd.to_datetime(aapl_minute['timestamp'])\n",
    "aapl_minute['date_only'] = aapl_minute['timestamp'].dt.date\n",
    "\n",
    "daily_prices = aapl_minute.sort_values('timestamp').groupby('date_only').agg(\n",
    "    open=('open', 'first'),\n",
    "    close=('close', 'last')\n",
    ").reset_index()\n",
    "\n",
    "trades = pd.read_csv(\"trades_over_15k_filtered.csv\", parse_dates=['tx_date'])\n",
    "trades_aapl = trades[trades['ticker'].str.upper() == 'AAPL'].copy()\n",
    "trades_aapl['trade_date'] = trades_aapl['tx_date'].dt.date\n",
    "\n",
    "3a️⃣ Filter trades between 01.01.2022 and 30.06.2025\n",
    "start_date = pd.to_datetime(\"2022-01-01\").date()\n",
    "end_date = pd.to_datetime(\"2025-06-30\").date()\n",
    "trades_aapl = trades_aapl[(trades_aapl['trade_date'] >= start_date) & (trades_aapl['trade_date'] <= end_date)]\n",
    "\n",
    "Merge trades with daily Apple prices\n",
    "df = trades_aapl.merge(daily_prices, left_on='trade_date', right_on='date_only', how='left')\n",
    "\n",
    "df['movement'] = df.apply(lambda row: 'Up' if row['close'] > row['open'] else 'Down', axis=1)\n",
    "\n",
    "\n",
    "#sold when up, bought when down\n",
    "def flag_trade(row):\n",
    "    if row['order_type'].lower().startswith('sale') and row['movement'] == 'Up':\n",
    "        return 'Sold while Up'\n",
    "    elif row['order_type'].lower().startswith('purchase') and row['movement'] == 'Down':\n",
    "        return 'Bought while Down'\n",
    "    else:\n",
    "        return 'Aligned'\n",
    "\n",
    "\n",
    "df['trade_flag'] = df.apply(flag_trade, axis=1)\n",
    "\n",
    "\n",
    "df.to_csv(\"apple_trades_with_daily_movement_2022_H1.csv\", index=False)\n",
    "\n",
    "\n",
    "df[['first_name','last_name', 'tx_date', 'order_type', 'tx_amount', 'open', 'close', 'movement', 'trade_flag']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
